\documentclass[
    12pt,
    letterpaper,
    aps,
    prd,
    longbibliography,
    nofootinbib,
    raggedbottom,
    amsmath,
    amssymb,
    amsfonts,
    nobibnotes,
]{revtex4-2}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{bm}

\newcommand{\m}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\tx}[1]{\text{#1}}
\newcommand{\pn}[1]{\left(#1\right)}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\bk}[1]{\left[#1\right]}
\newcommand{\abk}[1]{\left\langle#1\right\rangle}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\ellipsis}{\,\ldots}
\newcommand{\given}{\,|\,}
\newcommand{\where}{\mid}
\newcommand{\bbm}[1]{\mathbb{#1}}
\newcommand{\impl}{\rightarrow}
\newcommand{\dubimpl}{\leftrightarrow}
\newcommand{\calm}[1]{\mathcal{#1}}
\newcommand{\obar}[1]{\overline{#1}}
\newcommand{\lcm}{\text{lcm}}
\newcommand{\image}{\text{image}}
\newcommand{\iso}{\cong}
\newcommand{\sub}{\subseteq}
\newcommand{\comp}{\circ}
\newcommand{\al}{\forall}
\newcommand{\ex}{\exists}
\newcommand{\integ}{\int}
\newcommand{\gam}{\gamma}
\newcommand{\del}{\delta}
\newcommand{\Del}{\Delta}
\newcommand{\lam}{\lambda}
\newcommand{\Lam}{\Lambda}
\newcommand{\eps}{\epsilon}
\newcommand{\om}{\omega}
\newcommand{\Om}{\Omega}

\newcommand{\ensurebbm}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\C}{\ensurebbm{C}}
\newcommand{\Z}{\ensurebbm{Z}}
\newcommand{\R}{\ensurebbm{R}}
\newcommand{\N}{\ensurebbm{N}}
\newcommand{\Q}{\ensurebbm{Q}}
\newcommand{\F}{\ensurebbm{F}}

\let\origfootnote\footnote
\renewcommand{\footnote}[1]{%
   \begingroup%
   \renewcommand{\footnotesize}{\fontsize{10pt}{8pt}\selectfont}%
   \origfootnote{#1}%
   \endgroup%
}

\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

\begin{document}

\title{Risks from learned optimization \\ in advanced machine learning systems}
\author{Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant}
\affiliation{with special thanks to Paul Christiano, Eric Drexler, Jan Leike, Rohin Shah, the MIRI agent foundations team, and everyone else who provided feedback on earlier versions of this paper.}
\date{\today}

\begin{abstract}
We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizerâ€”a situation we refer to as mesa-optimization. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers? Second, when a learned model is an optimizer, what will its objective be, and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.
\end{abstract}

\maketitle

\newpage
\tableofcontents
\newpage

\section{Introduction}

BODY.

\nocite{leike}
\nocite{*}

\bibliography{inner_optimization_sources}

\end{document}
